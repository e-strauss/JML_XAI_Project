var documenterSearchIndex = {"docs":
[{"location":"api/#Base-Functions","page":"Important Functions","title":"Base Functions","text":"","category":"section"},{"location":"api/#XAIBase.jl-interface-for-calling-LIME-and-SHAP","page":"Important Functions","title":"XAIBase.jl interface for calling LIME and SHAP","text":"","category":"section"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"JML_XAI_Project.LIME\nJML_XAI_Project.LIME(::Any, ::JML_XAI_Project.AbstractOutputSelector)","category":"page"},{"location":"api/#JML_XAI_Project.LIME","page":"Important Functions","title":"JML_XAI_Project.LIME","text":"(method::LIME)(input, output_selector::AbstractOutputSelector)\n\nThis function applies the LIME (Local Interpretable Model-agnostic Explanations) method to a given input and returns an explanation of the model's prediction.\n\nArguments\n\ninput: The input data for the model, typically a 4D array (e.g., image data with dimensions height x width x channels x batch size).\noutput_selector::AbstractOutputSelector: An object that selects specific outputs from the model's predictions.\n\nReturns\n\nExplanation: An object containing the following fields:\nval: The explanation value generated by the explain_instance function.\noutput: The output of the model for the given input.\noutput_selection[1]: The selected output used for generating the explanation.\n:MyMethod: The method used for generating the explanation.\n:attribution: The type of explanation provided (attribution).\nextras: Additional information related to the explanation (currently set to nothing).\n\n\n\n\n\n","category":"type"},{"location":"api/#JML_XAI_Project.LIME-Tuple{Any, XAIBase.AbstractOutputSelector}","page":"Important Functions","title":"JML_XAI_Project.LIME","text":"(method::LIME)(input, output_selector::AbstractOutputSelector)\n\nThis function applies the LIME (Local Interpretable Model-agnostic Explanations) method to a given input and returns an explanation of the model's prediction.\n\nArguments\n\ninput: The input data for the model, typically a 4D array (e.g., image data with dimensions height x width x channels x batch size).\noutput_selector::AbstractOutputSelector: An object that selects specific outputs from the model's predictions.\n\nReturns\n\nExplanation: An object containing the following fields:\nval: The explanation value generated by the explain_instance function.\noutput: The output of the model for the given input.\noutput_selection[1]: The selected output used for generating the explanation.\n:MyMethod: The method used for generating the explanation.\n:attribution: The type of explanation provided (attribution).\nextras: Additional information related to the explanation (currently set to nothing).\n\n\n\n\n\n","category":"method"},{"location":"api/#Generation-of-a-new-interpretable-data-set","page":"Important Functions","title":"Generation of a new interpretable data set","text":"","category":"section"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"Conversion of the input image into mulitple disturbed interpretable representations","category":"page"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"JML_XAI_Project.explain_instance \nJML_XAI_Project.default_segmentation_function\nJML_XAI_Project.cosine_similiarity \nJML_XAI_Project.pairwise_distance \nJML_XAI_Project.data_labels  \nJML_XAI_Project.euclidian_distance \nJML_XAI_Project.exponential_kernel \nJML_XAI_Project.create_fudged_image ","category":"page"},{"location":"api/#JML_XAI_Project.explain_instance","page":"Important Functions","title":"JML_XAI_Project.explain_instance","text":"explain_instance(image, classifier_fn, output_selection, num_features=16, num_samples=64, batch_size=5, distance_metric=\"cosine\",)\n\nGenerates explanations for a prediction.\n\nwe generate neighborhood data by randomly perturbing features from the instance.\nWe then learn locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.\n\nParameters\n\nimage:  4 dimension RGB image. \nclassifier_fn: classifier prediction probability function, which takes a image Matrix and outputs prediction probabilities.           \nlabels: iterable with labels to be explained.          \nnum_features: maximum number of features present in explanation, default = 16.\nnum_samples: size of the neighborhood (perturbed images) to learn the linear model, default = 64.\nbatch_size: batch size for model predictions, default = 5.\ndistance_metric: the distance metric to use for weights, default = 'cosine'\n\nReturns:\n\nAn ImageExplanation object with the corresponding explanations.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.default_segmentation_function","page":"Important Functions","title":"JML_XAI_Project.default_segmentation_function","text":"function default_segmentation_function(algo_type::String)\n\nReturn image segmantation function, if no function was passed originally. Based on Scikit-Image implementation.\n\nfelzenszwalb:\n\nfurther explainations: https://www.analyticsvidhya.com/blog/2021/05/image-segmentation-with-felzenszwalbs-algorithm/\n\nParameters\n\nalgo_type: string, segmentation algorithm among the following:       \"felzenszwalb\"\n\nReturns\n\nsegmentation_func::Function: A segmantation function.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.cosine_similiarity","page":"Important Functions","title":"JML_XAI_Project.cosine_similiarity","text":"cosine_similarity(A::AbstractArray, B::AbstractArray)\n\nComputes the cosine similarity between corresponding rows of two arrays.\n\nParameters\n\nA::AbstractArray: The first input array. Each row represents a vector.\nB::AbstractArray: The second input array. Each row represents a vector. Must have the same number of columns as A.\n\nReturns\n\nAbstractArray: An array of cosine similarities between corresponding rows of A and B.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.pairwise_distance","page":"Important Functions","title":"JML_XAI_Project.pairwise_distance","text":"pairwise_distance(A::AbstractArray, B::AbstractArray; method=\"cosine\")\n\nComputes the pairwise distance between corresponding rows of two arrays using the specified distance metric.\n\nParameters\n\nA::AbstractArray: The first input array. Each row represents a vector.\nB::AbstractArray: The second input array. Each row represents a vector. Must have the same number of columns as A.\nmethod::String=\"cosine\": The distance metric to use. Options are \"cosine\" for cosine similarity or \"euclidean\" for Euclidean distance. Defaults to \"cosine\".\n\nReturns\n\nAbstractArray: An array of distances between corresponding rows of A and B.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.data_labels","page":"Important Functions","title":"JML_XAI_Project.data_labels","text":"function data_labels(image::Matrix{RGB{<:AbstractFloat}}, fudged_image::Matrix{RGB{<:AbstractFloat}}, segments::Matrix{<:Integer}, classifier_fn::Function, num_samples<:Integer, batch_size<:Integer=10)\n\nGenerates perturbed versions of a given image by turning superpixels on or off,using a specified  segmentation map. It then predicts the class probabilities for these perturbed images using a provided  classifier function. The function returns a tuple containing the binary matrix of perturbed images (data)  and their corresponding prediction probabilities (labels). This is useful for techniques like LIME to  understand and explain model predictions locally.\n\nParameters\n\nimage::Matrix{RGB{Float32}}: The original input image.\nfudged_image::Matrix{RGB{Float32}}: The fudged image with mean colors for segments.\nsegments::Matrix{Int}: The segmentation map where each segment is labeled with an integer.\nclassifier_fn::Function: A function that takes a batch of images and returns their predicted labels.\nnum_samples::Int:  The number of samples to generate.\nbatch_size::Int=10: The size of the batches to process through the classifier function. Defaults to 10.\n\nReturns\n\ndata::Matrix{Int}: A binary matrix where each row indicates which features (segments) are active.\nlabels::Matrix{Float64}: A matrix of classifier predictions for the corresponding images in data.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.euclidian_distance","page":"Important Functions","title":"JML_XAI_Project.euclidian_distance","text":"function euclidian_distance(A,B)\n\ncalculates the euclidian distance between each column vector in input matrix A and the column vectors in input matrix B\n\nParameters:\n\nA:  matrix (m,n)\nB:  matrix (m,n) or (1,n)\n\nReturns:\n\ndistance: 1-d array of distances\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.exponential_kernel","page":"Important Functions","title":"JML_XAI_Project.exponential_kernel","text":"exponential_kernel(d::AbstractArray; kernel_width=0.25)\n\nComputes the exponential kernel for a given array of distances.\n\nParameters\n\nd::AbstractArray: An array of distances.\nkernel_width::Float64=0.25: The width of the kernel. Defaults to 0.25.\n\nReturns\n\nAbstractArray: An array of kernel values computed from the input distances.\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.create_fudged_image","page":"Important Functions","title":"JML_XAI_Project.create_fudged_image","text":"create_fudged_image(img::Matrix{RGB{Float32}}, seg_map::Matrix{<:Integer})\n\nCreates a \"fudged\" image where the pixels of each segment are replaced by the mean color of that segment.\n\nParameters\n\nimg::Matrix{RGB{Float32}}: The input image as a matrix of RGB colors with floating point values.\nseg_map::Matrix{<:Integer}: The segmentation map containing the segment labels for each pixel.\n\nReturns\n\nMatrix{RGB{Float32}}: A new image where the pixels of each segment are replaced by the mean color of that segment.\n\n\n\n\n\ncreate_fudged_image(img::Matrix{Float32}, seg_map::Matrix{<:Integer})\n\nCreates a \"fudged\" image where the pixels of each segment are replaced by the mean color of that segment.\n\nParameters\n\nimg::Matrix{Float32}: The input image as a matrix of RGB colors with floating point values.\nseg_map::Matrix{<:Integer}: The segmentation map containing the segment labels for each pixel.\n\nReturns\n\nMatrix{RGB{Float32}}: A new image where the pixels of each segment are replaced by the mean color of that segment.\n\n\n\n\n\n","category":"function"},{"location":"api/#Feature-selection-and-train-simplified-model-on-the-interpretable-data-set","page":"Important Functions","title":"Feature selection and train simplified model on the interpretable data set","text":"","category":"section"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"JML_XAI_Project.explain_instance_with_data \nJML_XAI_Project.weighted_data \nJML_XAI_Project.train_ridge_regressor\nJML_XAI_Project.feature_selection","category":"page"},{"location":"api/#JML_XAI_Project.explain_instance_with_data","page":"Important Functions","title":"JML_XAI_Project.explain_instance_with_data","text":"function explain_instance_with_data(neighborhood_data, neighborhood_labels, distances, label, num_features, kernel_fn = (x) -> 1 .- x)\n\nTakes perturbed data, labels and distances, returns explanation.\n\nParameters\n\nneighborhood_data: perturbed data\nneighborhood_labels: corresponding perturbed labels. should have as many columns as the number of possible labels.\ndistances: distances to original data point.\nkernel_fn: (similiarity) kernel function that transforms an array of distances into an array of proximity values (floats)\nlabel: label for which we want an explanation\nnum_features: maximum number of features in explanation\nmodel_regressor: sklearn regressor to use in explanation. Defaults to Ridge regression if None. Must have modelregressor.coef and 'sampleweight' as a parameter to modelregressor.fit()\n\nReturns\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.weighted_data","page":"Important Functions","title":"JML_XAI_Project.weighted_data","text":"normalize_data(X::Matrix, y::Vector, weights::Vector)\n\nReturns the weight normalization of features and labels using a weight vector.\n\nParameters\n\nX: features\ny: labels\n\n-weights: weight vector\n\nReturns\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.train_ridge_regressor","page":"Important Functions","title":"JML_XAI_Project.train_ridge_regressor","text":"train_ridge_regressor(X::Matrix{<:Real}, y::Vector{<:Real}; lam::Real, weights::Vector{Real})\n\nReturns the trained simplified linear model as a matrix using ridge regression:\n\nParameters\n\nX::Matrix{<:Real}: Simplified features\ny::Vector{<:Real}: Corresponding labels\n\nReturns\n\nVector{Float64}: Simplified linear model\n\n\n\n\n\n","category":"function"},{"location":"api/#JML_XAI_Project.feature_selection","page":"Important Functions","title":"JML_XAI_Project.feature_selection","text":"feature_selection(X::Matrix, y::Vector, max_feat::Int) -> ReturnType\n\nSelects features for the model using LARS with Lasso [https://tibshirani.su.domains/ftp/lars.pdf] s.t. len(selectedfeatures) <= maxfeat Use LARS package: https://github.com/simonster/LARS.jl\n\nParameters\n\nX: weighted features\ny: weighted labels\n\nReturns\n\nindices of selected features\n\n\n\n\n\n","category":"function"},{"location":"api/#Shap","page":"Important Functions","title":"Shap","text":"","category":"section"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"JML_XAI_Project.agnostic_kernel","category":"page"},{"location":"api/#JML_XAI_Project.agnostic_kernel","page":"Important Functions","title":"JML_XAI_Project.agnostic_kernel","text":"agnostic_kernel(simpleFeatures::Matrix{<:Real})\n\nCalculates the weights used for building the ridge regression model (simplified model),     when applying SHAP with a model-agnostic kernel.     Implentation based on \"A Unified Approach to Interpreting Model Predictions\" (https://arxiv.org/abs/1705.07874)\n\nParameters\n\nsimpleFeatures::Matrix{<:Real}: The simplified features used for building the simlified model.\n\nReturns\n\nVector{Float64}: A vector of weights with one weight for each simplified sample.\n\n\n\n\n\n","category":"function"},{"location":"api/#Index","page":"Important Functions","title":"Index","text":"","category":"section"},{"location":"api/","page":"Important Functions","title":"Important Functions","text":"","category":"page"},{"location":"lime/#LIME-Example","page":"Example","title":"LIME Example","text":"","category":"section"},{"location":"lime/","page":"Example","title":"Example","text":"For the example, we load an image from the imagenet-sample-images repository. An explanation will be generated for the image later on. ","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"using ExplainableAI\nusing Metalhead: ResNet\nusing JML_XAI_Project\nusing Images\nusing VisionHeatmaps\n\nimg = load(\"images/boa_constrictor.JPEG\")","category":"page"},{"location":"lime/#Image-pre-processing","page":"Example","title":"Image pre-processing","text":"","category":"section"},{"location":"lime/","page":"Example","title":"Example","text":"The image is processed in order to use it as input for a model.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"imgVec = permutedims(channelview(img),(3,2,1)); \nimgVec = reshape(imgVec, size(imgVec)..., 1);\ninput = Float32.(imgVec);\nnothing # hide","category":"page"},{"location":"lime/#Generation-of-the-explanation","page":"Example","title":"Generation of the explanation","text":"","category":"section"},{"location":"lime/","page":"Example","title":"Example","text":"The next step is to initialize a pre-trained ResNet model and apply LIME to it.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"info: Info\nAny classifier or regressor can be used at this point.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"model = ResNet(18; pretrain = true);\nmodel = model.layers;\nanalyzer = LIME(model);\nexpl = analyze(input, analyzer);","category":"page"},{"location":"lime/#Visualize-explanaition","page":"Example","title":"Visualize explanaition","text":"","category":"section"},{"location":"lime/","page":"Example","title":"Example","text":"The generated explanation can now be displayed as a heat map.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"using VisionHeatmaps\nheatmap(expl.val)","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"info: Info\nThe following code generates a clearer representation of the explanation.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"function generate_heatmap(map; img=nothing, overlay=false, blurring=false, gaussSTD=2)\n    map = heatmap(map.val)\n\n    if blurring == true\n        gaussKern2 = ImageFiltering.KernelFactors.gaussian((gaussSTD,gaussSTD))\n        map = ImageFiltering.imfilter(map, gaussKern2)\n    end\n\n    if overlay == true\n        map = (0.5.*Gray.(img) + 0.5.*map)\n    end\n\n    return map\nend","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"generate_heatmap(expl, img=img, overlay=true, blurring=true)","category":"page"},{"location":"lime/#Label","page":"Example","title":"Label","text":"","category":"section"},{"location":"lime/","page":"Example","title":"Example","text":"To find out the generated corresponding label of the image, we output the number of the label, which can be looked up in the linked text file.","category":"page"},{"location":"lime/","page":"Example","title":"Example","text":"print(\"Label: \", argmax(expl.output[:,1]) - 1)","category":"page"},{"location":"limeExp/#About-LIME","page":"About LIME","title":"About LIME","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"LIME (Local Interpretable Model-agnostic Explanations) approximates a complex model locally by using perturbed inputs (i.e. slightly modified versions of the original data) and training an interpretable model to explain the predictions of the complex model in the neighbourhood of a particular data point.","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"There are numerous variations in the implementation of LIME. For instance, linear models can be utilized as interpretable models, but decision trees are also a viable option. Additionally, there are various approaches to feature selection, which is an integral part of generating the explanation.","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"The JML package is based on the LIME package, which implements different variations of LIME in Python. For the JMLXAIProject package it was decided to follow the recommendation from this paper, i.e. Ridge Regression is used as the interpretable model and Lasso for feature selection.","category":"page"},{"location":"limeExp/#How-does-the-code-implements-LIME-for-Image-Input:","page":"About LIME","title":"How does the code implements LIME for Image Input:","text":"","category":"section"},{"location":"limeExp/#1-Create-super-pixels-for-the-input-image-through-segmentation","page":"About LIME","title":"1 Create super pixels for the input image through segmentation","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"The aim of LIME is to generate an interpretable explanation, the first step is to calculate the input image in super pixels. ","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"Original input Output\n(Image: ) (Image: )","category":"page"},{"location":"limeExp/#2-Create-different-disturbed-versions-of-the-super-pixels-input-image","page":"About LIME","title":"2 Create different disturbed versions of the super pixels input image","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"Using a mask (a binary vector), the super pixels can then be faded in and out, 1 means super pixel is shown, 0 means no colour value, thus generating an interpretable representation of the image.","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"super pixels input disturbed versions 1 disturbed versions 2 disturbed versions 3\n(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"limeExp/#3-Predict-the-class-probabilities-of-disturbed-images-with-classificator","page":"About LIME","title":"3 Predict the class probabilities of disturbed images with classificator","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/#4-Calculate-distance-of-features-of-super-pixel-image-and-disturbed-images","page":"About LIME","title":"4 Calculate distance of features of super pixel image and disturbed images","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/#5-Calculate-Explanation-calculate-weights-of-each-superpixel:","page":"About LIME","title":"5 Calculate Explanation - calculate weights of each superpixel:","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"kernel function is applyed on the calculated distances used as weights\nnormalize features and labels using weights\nselect n features with lasso\ncalculate ridge regression model as simplified model with selected features and distance weights\ncreate a vector with the length corresponding to the number of features, in place of the selected features store their ridge regression coefficients, the other entries are 0","category":"page"},{"location":"limeExp/#6-Give-each-segment-in-the-image-its-calculated-weight","page":"About LIME","title":"6 Give each segment in the image its calculated weight","text":"","category":"section"},{"location":"limeExp/","page":"About LIME","title":"About LIME","text":"","category":"page"},{"location":"limeExp/#7-Process-the-information-with-XAIBase.jl-to-display-the-explanation-as-a-heat-map","page":"About LIME","title":"7 Process the information with XAIBase.jl to display the explanation as a heat map","text":"","category":"section"},{"location":"shap/#SHAP-Example","page":"Example","title":"SHAP Example","text":"","category":"section"},{"location":"shap/","page":"Example","title":"Example","text":"The SHAP call is very similar to the LIME call. For the example, we load an image from the imagenet-sample-images repository. An explanation will be generated for the image later on. ","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"using ExplainableAI\nusing Metalhead: ResNet\nusing JML_XAI_Project\nusing Images\nusing VisionHeatmaps\n\nimg = load(\"images/boa_constrictor.JPEG\")","category":"page"},{"location":"shap/#Image-pre-processing","page":"Example","title":"Image pre-processing","text":"","category":"section"},{"location":"shap/","page":"Example","title":"Example","text":"The image is processed in order to use it as input for a model.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"imgVec = permutedims(channelview(img),(3,2,1)); \nimgVec = reshape(imgVec, size(imgVec)..., 1);\ninput = Float32.(imgVec);\nnothing # hide","category":"page"},{"location":"shap/#Generation-of-the-explanation","page":"Example","title":"Generation of the explanation","text":"","category":"section"},{"location":"shap/","page":"Example","title":"Example","text":"The next step is to initialize a pre-trained ResNet model and apply SHAP to it.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"info: Info\nAny classifier or regressor can be used at this point.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"model = ResNet(18; pretrain = true);\nmodel = model.layers;\n\n#A different kernel function is used to call SHAP, \n#and no feature selection is required, which is why LASSO must be set to false\nanalyzer = LIME(model, agnostic_kernel, false)\nexpl = analyze(input, analyzer);","category":"page"},{"location":"shap/#Visualize-explanaition","page":"Example","title":"Visualize explanaition","text":"","category":"section"},{"location":"shap/","page":"Example","title":"Example","text":"The generated explanation can now be displayed as a heat map.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"using VisionHeatmaps\nheatmap(expl.val)","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"info: Info\nThe following code generates a clearer representation of the explanation.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"function generate_heatmap(map; img=nothing, overlay=false, blurring=false, gaussSTD=2)\n    map = heatmap(map.val)\n\n    if blurring == true\n        gaussKern2 = ImageFiltering.KernelFactors.gaussian((gaussSTD,gaussSTD))\n        map = ImageFiltering.imfilter(map, gaussKern2)\n    end\n\n    if overlay == true\n        map = (0.5.*Gray.(img) + 0.5.*map)\n    end\n\n    return map\nend","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"generate_heatmap(expl, img=img, overlay=true, blurring=true)","category":"page"},{"location":"shap/#Label","page":"Example","title":"Label","text":"","category":"section"},{"location":"shap/","page":"Example","title":"Example","text":"To find out the generated corresponding label of the image, we output the number of the label, which can be looked up in the linked text file.","category":"page"},{"location":"shap/","page":"Example","title":"Example","text":"print(\"Label: \", argmax(expl.output[:,1]) - 1)","category":"page"},{"location":"shapExp/#About-SHAP","page":"About SHAP","title":"About SHAP","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"The implementation of SHAP (SHapley Additive ExPlanations) incorporates the idea of Shapley values, which have their origins in game theory. Shapley values indicate how much a player has contributed to the outcome of a group game. This idea has been adapted to explainable AI methods by evaluating how much each feature contributes to the model's prediction, such as in the recognition of an image.","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"The paper, which introduces the idea of SHAP, presents different variations of SHAP. The JMLXAIPacket implements Kernel SHAP, which is a model-agnostic variant, as it can be easily integrated into the LIME code and is universally applicable in contrast to model-specific variants of SHAP.","category":"page"},{"location":"shapExp/#How-does-the-code-implement-SHAP-for-image-input:","page":"About SHAP","title":"How does the code implement SHAP for image input:","text":"","category":"section"},{"location":"shapExp/#1-Create-super-pixels-for-the-input-image-through-segmentation","page":"About SHAP","title":"1 Create super pixels for the input image through segmentation","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"The aim of LIME is to generate an interpretable explanation, the first step is to calculate the input image in super pixels. ","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"Original input Output\n(Image: ) (Image: )","category":"page"},{"location":"shapExp/#2-Create-different-disturbed-versions-of-the-super-pixels-input-image","page":"About SHAP","title":"2 Create different disturbed versions of the super pixels input image","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"Using a mask (a binary vector), the super pixels can then be faded in and out, 1 means super pixel is shown, 0 means no colour value, thus generating an interpretable representation of the image.","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"super pixels input disturbed versions 1 disturbed versions 2 disturbed versions 3\n(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"shapExp/#3-Predict-the-class-probabilities-of-disturbed-images-with-classificator","page":"About SHAP","title":"3 Predict the class probabilities of disturbed images with classificator","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/#4-Calculate-shapley-weights-with-weighting-kernel:","page":"About SHAP","title":"4 Calculate shapley weights with weighting kernel:","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"Disturbed images in which very many or very few segments are masked in or out are given high weights, while images in which approximately half of the samples are masked in and out are given a weight close to zero. This way images, in which the influence of a single segment can be preciseley assessed, are higher weighted than those, for which this is not possible.","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"(Image: )","category":"page"},{"location":"shapExp/#5-Calculate-Explanation","page":"About SHAP","title":"5 Calculate Explanation","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"calculate ridge regression model as simplified model with shapley weights\ncreate a vector with the length corresponding to the number of features, in place of each feature store their ridge regression coefficients","category":"page"},{"location":"shapExp/#6-Give-each-segment-in-the-image-its-calculated-weight","page":"About SHAP","title":"6 Give each segment in the image its calculated weight","text":"","category":"section"},{"location":"shapExp/","page":"About SHAP","title":"About SHAP","text":"","category":"page"},{"location":"shapExp/#7-Process-the-information-with-XAIBase.jl-to-display-the-explanation-as-a-heat-map","page":"About SHAP","title":"7 Process the information with XAIBase.jl to display the explanation as a heat map","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = JML_XAI_Project","category":"page"},{"location":"#JML*XAI*Project-LIME-and-SHAP-for-Julia","page":"Home","title":"JMLXAIProject - LIME and SHAP for Julia","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements the explainable AI methods LIME and SHAP using XAIBase.jl. LIME and SHAP are model-agnostic explainable AI methods, so they can be used to explain different kinds of models. The JMLXAIproject package provides explanations for image inputs and visualizes them as heatmaps.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Input Output\n(Image: ) (Image: )","category":"page"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To familiarise yourself with the package, there are sample implementations of LIME & SHAP in the documentation. On the other hand, there is the subfolder \"demo\" in the JMLXAIProject package folder, from which \"lime_demo.jl\" can be executed, which runs LIME or SHAP.","category":"page"},{"location":"","page":"Home","title":"Home","text":"warning: Warning\nTo use the package please add the following code to your environment","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/e-strauss/JML_XAI_Project.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for JMLXAIProject.","category":"page"}]
}
